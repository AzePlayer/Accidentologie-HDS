
# projet.py ‚Äî Pr√©sentation des donn√©es
import os, io, csv
import numpy as np
import pandas as pd
import streamlit as st

# ---------------------------
# Config & titre
# ---------------------------
st.set_page_config(page_title="üì¶ Projet ‚Äî Pr√©sentation des donn√©es", page_icon="üì¶", layout="wide")
st.title("üì¶ Projet ‚Äî Pr√©sentation des donn√©es")
st.caption("Aper√ßu du fichier `acc.csv` : dictionnaire des variables (FR/CSV), types, valeurs manquantes et exploration d‚Äôune variable. "
           "Les autres pages conservent les noms techniques d‚Äôorigine.")

# ---------------------------
# Mapping colonnes FR (sp√©cifique √† cette page)
# ---------------------------
COLUMNS_FR = {
  "TYPE_COLLI": "Type de collision",
  "adresse": "Adresse",
  "age_usa1": "√Çge usager 1",
  "age_usa2": "√Çge usager 2",
  "age_usa3": "√Çge usager 3",
  "cat_route1": "Cat√©gorie route 1",
  "cat_route2": "Cat√©gorie route 2",
  "cat_ve1": "Cat√©gorie v√©hicule 1",
  "cat_ve2": "Cat√©gorie v√©hicule 2",
  "code_insee": "Code INSEE",
  "commune": "Commune",
  "cond_atmos": "Conditions atmosph√©riques",
  "date": "Date",
  "geo_point_2d": "G√©opoint (lat, lon)",
  "grav_usa1": "Gravit√© usager 1",
  "grav_usa2": "Gravit√© usager 2",
  "grav_usa3": "Gravit√© usager 3",
  "heure": "Heure",
  "heure_num": "Heure (num√©rique)",
  "id_pv": "Identifiant PV",
  "lieu": "Lieu",
  "luminosite": "Luminosit√©",
  "man_ve1": "Man≈ìuvre v√©hicule 1",
  "man_ve2": "Man≈ìuvre v√©hicule 2",
  "nb_bh_ve1": "Bless√©s hospitalis√©s (v√©h. 1)",
  "nb_bh_ve2": "Bless√©s hospitalis√©s (v√©h. 2)",
  "nb_bnh_ve1": "Bless√©s non hospitalis√©s (v√©h. 1)",
  "nb_bnh_ve2": "Bless√©s non hospitalis√©s (v√©h. 2)",
  "nb_pie": "Nombre de pi√©tons",
  "nb_t_ve1": "Nombre de tu√©s (v√©h. 1)",
  "nb_t_ve2": "Nombre de tu√©s (v√©h. 2)",
  "nb_usager": "Nombre d'usagers",
  "nb_veh": "Nombre de v√©hicules",
  "nom_route1": "Nom route 1",
  "nom_route2": "Nom route 2",
  "rev_route1": "R√©f√©rence route 1",
  "rev_route2": "R√©f√©rence route 2",
  "route_ve1": "Route v√©hicule 1",
  "route_ve2": "Route v√©hicule 2",
  "sens_ve1": "Sens de circulation (v√©h. 1)",
  "sens_ve2": "Sens de circulation (v√©h. 2)",
  "sexe_usa1": "Sexe usager 1",
  "sexe_usa2": "Sexe usager 2",
  "sexe_usa3": "Sexe usager 3",
  "type_acci": "Type d'accident",
  "usager1": "Cat√©gorie usager 1",
  "usager2": "Cat√©gorie usager 2",
  "usager3": "Cat√©gorie usager 3",
  "veh_usa1": "Type de v√©hicule (usager 1)",
  "veh_usa2": "Type de v√©hicule (usager 2)",
  "veh_usa3": "Type de v√©hicule (usager 3)",
  "victime": "Victime"
}

def tech2fr_name(col: str) -> str:
    return COLUMNS_FR.get(col, col)

def fr_choices_from_df(df: pd.DataFrame) -> list[str]:
    return [tech2fr_name(c) for c in df.columns]

def fr2tech_lookup(label_fr: str, df: pd.DataFrame) -> str:
    for c in df.columns:
        if tech2fr_name(c) == label_fr:
            return c
    return label_fr

# ---------------------------
# Lecture CSV robuste
# ---------------------------
def _read_csv_smart(buffer_or_path, default_sep=";", encodings=("utf-8", "latin-1")):
    """D√©tection ; / , et multiple encodages."""
    def _read_bytes(raw: bytes):
        for enc in encodings:
            try:
                sample = raw[:10000].decode(enc, errors="ignore")
                dialect = csv.Sniffer().sniff(sample, delimiters=",;")
                sep = dialect.delimiter if dialect.delimiter in [",", ";"] else default_sep
                return pd.read_csv(io.BytesIO(raw), sep=sep, encoding=enc, low_memory=False)
            except Exception:
                continue
        return pd.read_csv(io.BytesIO(raw), sep=default_sep, low_memory=False)

    if isinstance(buffer_or_path, (str, os.PathLike)):
        if not os.path.exists(buffer_or_path):
            return pd.DataFrame()
        with open(buffer_or_path, "rb") as f:
            raw = f.read()
        return _read_bytes(raw)
    else:
        raw = buffer_or_path.read()
        return _read_bytes(raw)

@st.cache_data(show_spinner=False)
def load_data(default_path="acc.csv") -> pd.DataFrame:
    df = _read_csv_smart(default_path)
    if df.empty:
        return df
    # normalisations minimales
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"], errors="coerce", dayfirst=True)
    if "heure" in df.columns and "heure_num" not in df.columns:
        h = df["heure"].astype(str).str.replace("h", ":", regex=False)
        df["heure_num"] = pd.to_datetime(h, errors="coerce").dt.hour
    for c in ("latitude", "longitude"):
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df

# ---------------------------
# Sidebar : import optionnel
# ---------------------------
st.sidebar.header("‚öôÔ∏è Donn√©es")
upl = st.sidebar.file_uploader("Importer un CSV (optionnel)", type=["csv"])
df = _read_csv_smart(upl) if upl is not None else load_data("acc.csv")

if df.empty:
    st.error("Aucune donn√©e charg√©e. Place `acc.csv` √† la racine du projet ou importe un CSV via la barre lat√©rale.")
    st.stop()

st.success(f"‚úÖ Donn√©es charg√©es : **{len(df):,}** lignes √ó **{df.shape[1]}** colonnes".replace(",", " "))

# ============================================================
# 1) Dictionnaire des variables (FR + nom d‚Äôorigine)
# ============================================================
st.subheader("üìö Dictionnaire des variables")

col_opt1, col_opt2, col_opt3 = st.columns([1, 1, 1.2])
with col_opt1:
    show_preview = st.toggle("Aper√ßu concis (recommand√©)", value=True,
                             help="Top 1‚Äì3 modalit√©s ou stats courtes.")
with col_opt2:
    sort_by = st.selectbox("Trier par :", ["Ordre CSV", "Nom (FR)", "Type", "Nb modalit√©s", "% manquants"], index=0)
with col_opt3:
    search = st.text_input("üîé Filtrer (FR/CSV contient‚Ä¶)", "")

def _infer_kind(s: pd.Series):
    if pd.api.types.is_numeric_dtype(s): return "Quantitative", "num√©rique"
    if pd.api.types.is_bool_dtype(s):    return "Qualitative", "bool√©enne"
    if pd.api.types.is_datetime64_any_dtype(s): return "Quantitative", "date/temps"
    nun = s.dropna().nunique()
    if nun <= 25 or pd.api.types.is_categorical_dtype(s) or pd.api.types.is_object_dtype(s):
        return "Qualitative", "cat√©gorielle"
    return "Qualitative", "texte libre"

def _preview(s: pd.Series, concise: bool = True) -> str:
    s_nonan = s.dropna()
    if s_nonan.empty: return "‚Äî"
    if pd.api.types.is_numeric_dtype(s):
        if s_nonan.size >= 3:
            if concise:
                return f"min‚âà{s_nonan.min():.2f} | med‚âà{np.median(s_nonan):.2f} | max‚âà{s_nonan.max():.2f}"
            q5, q50, q95 = np.nanpercentile(s_nonan.astype(float), [5, 50, 95])
            return f"q5‚âà{q5:.2f} | q50‚âà{q50:.2f} | q95‚âà{q95:.2f}"
        return f"moy‚âà{s_nonan.astype(float).mean():.2f}"
    vc = s_nonan.astype(str).value_counts().head(3 if concise else 5)
    return " | ".join([f"{k} ({v})" for k, v in vc.items()])

original_cols = list(df.columns)
rows = []
for i, col in enumerate(original_cols, start=1):
    s = df[col]
    typ, sub = _infer_kind(s)
    nmods = int(s.dropna().nunique())
    miss_pct = 100 * s.isna().mean()
    rows.append({
        "Ordre CSV": i,
        "Nom d‚Äôorigine (CSV)": col,
        "Nom (FR)": tech2fr_name(col),
        "Type": typ,
        "Sous-type": sub,
        "Nb modalit√©s (ou valeurs uniques)": nmods,
        "% manquants": round(miss_pct, 2),
        "Aper√ßu": _preview(s, concise=show_preview)
    })

dict_df = pd.DataFrame(rows)

if search:
    m = (
        dict_df["Nom (FR)"].str.contains(search, case=False, na=False) |
        dict_df["Nom d‚Äôorigine (CSV)"].str.contains(search, case=False, na=False)
    )
    dict_df = dict_df[m]

if sort_by == "Ordre CSV":
    dict_df = dict_df.sort_values("Ordre CSV")
elif sort_by == "Nom (FR)":
    dict_df = dict_df.sort_values(["Nom (FR)", "Nom d‚Äôorigine (CSV)"])
elif sort_by == "Type":
    dict_df = dict_df.sort_values(["Type", "Nom (FR)"])
elif sort_by == "Nb modalit√©s":
    dict_df = dict_df.sort_values("Nb modalit√©s (ou valeurs uniques)", ascending=False)
else:
    dict_df = dict_df.sort_values("% manquants", ascending=False)

st.dataframe(
    dict_df[[
        "Ordre CSV",
        "Nom d‚Äôorigine (CSV)",
        "Nom (FR)",
        "Type",
        "Sous-type",
        "Nb modalit√©s (ou valeurs uniques)",
        "% manquants",
        "Aper√ßu"
    ]],
    use_container_width=True,
    hide_index=True
)

non_map = [c for c in original_cols if tech2fr_name(c) == c]
if non_map:
    st.caption(f"‚ö†Ô∏è Colonnes sans libell√© FR (affich√©es telles quelles) : {', '.join(non_map[:8])}{'‚Ä¶' if len(non_map)>8 else ''}")

st.divider()

# ============================================================
# 2) Exploration d√©taill√©e (une variable √† la fois, sans graphiques)
# ============================================================
st.subheader("üîé Exploration d√©taill√©e (une variable √† la fois)")

with st.expander("D√©plier l‚Äôexploration d√©taill√©e", expanded=False):
    left, right = st.columns([1.2, 1])

    with left:
        fr_choices = sorted(fr_choices_from_df(df))
        fr_default = tech2fr_name(df.columns[0]) if len(df.columns) else ""
        fr_label = st.selectbox("Choisir une variable", fr_choices,
                                index=fr_choices.index(fr_default) if fr_default in fr_choices else 0)
        var = fr2tech_lookup(fr_label, df)
        s = df[var]
        typ, sub = _infer_kind(s)
        n_unique = int(s.dropna().nunique())
        miss_pct = 100 * s.isna().mean()

        with st.container(border=True):
            st.markdown(f"**Nom (FR) :** `{fr_label}`  \n**Nom d‚Äôorigine (CSV) :** `{var}`")
            st.markdown(f"- **Type :** {typ}  ‚Ä¢  **Sous-type :** {sub}")
            st.markdown(f"- **Valeurs uniques :** {n_unique:,}".replace(",", " "))
            st.markdown(f"- **% manquants :** {miss_pct:.2f}%")

        st.markdown("#### Description")
        if pd.api.types.is_numeric_dtype(s):
            desc = s.describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95]).to_frame("Valeur")
            st.dataframe(desc, use_container_width=True)
        elif pd.api.types.is_datetime64_any_dtype(s):
            nonan = s.dropna()
            if nonan.empty:
                st.info("Aucune date exploitable.")
            else:
                st.write({
                    "min": str(nonan.min()),
                    "max": str(nonan.max()),
                    "nb. jours couverts (approx.)": int((nonan.max() - nonan.min()).days)
                })
        else:
            k = st.slider("Top K modalit√©s", 5, 30, 15, key="topk_cat_overview")
            vc = s.astype(str).value_counts(dropna=False).head(k).reset_index()
            vc.columns = [fr_label, "Effectif"]
            st.dataframe(vc, use_container_width=True)

    with right:
        st.markdown("#### √âchantillon (nettoy√©)")
        sample_size = st.slider("Taille de l‚Äô√©chantillon", 5, 50, 10, key="samp_overview")
        ex = (df[[var]].dropna().astype(str).head(sample_size))
        ex.columns = [fr_label]
        st.dataframe(ex, use_container_width=True, hide_index=True)

st.divider()

# ============================================================
# 3) Export l√©ger (optionnel)
# ============================================================
col_a, col_b = st.columns(2)
col_a.download_button(
    "üíæ T√©l√©charger le dictionnaire (CSV)",
    data=dict_df.to_csv(index=False).encode("utf-8"),
    file_name="dictionnaire_variables_fr.csv",
    mime="text/csv",
    use_container_width=True
)
col_b.download_button(
    "üíæ T√©l√©charger un √©chantillon (500 lignes)",
    data=df.head(500).to_csv(index=False).encode("utf-8"),
    file_name="echantillon_500.csv",
    mime="text/csv",
    use_container_width=True
)
